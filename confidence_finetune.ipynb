{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "import random\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_TOKEN'] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model(model_id,is_8bit = True):\n",
    "    if is_8bit:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            bnb_8bit_use_double_quant=True,\n",
    "            bnb_8bit_quant_type=\"nf4\",\n",
    "            bnb_8bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "    else:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config = bnb_config,\n",
    "        device_map = \"auto\",\n",
    "        cache_dir = \"cache\"\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_tokenizer(model_id,stop_tokens=True):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.pad_token_id = (\n",
    "            tokenizer.eos_token_id\n",
    "        )    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 1000\n",
    "def tokenize(tokenizer, prompt, add_eos_token=True):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    if (\n",
    "        result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "        and len(result[\"input_ids\"]) < MAX_LEN\n",
    "        and add_eos_token\n",
    "    ):\n",
    "        result[\"input_ids\"][MAX_LEN-1] = tokenizer.eos_token_id\n",
    "        result[\"attention_mask\"][MAX_LEN-1] = 1\n",
    "    \n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BLACK_BOX = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_black = \"\"\"\n",
    "Capture entity values from the LAST UTTERANCE of the conversation.\n",
    "FOCUS ONLY ON THE VALUES MENTIONED IN THE LAST UTTERANCE.\n",
    "Format the output as a valid JSON object, and for each entity-value pair, along with their pair-level confidence (0-1).\n",
    "Format: {{\"state\": {{\"_entity_\":\"_value_\"}}, \"confidence\": \"X\"}}\n",
    "Fill the actual entity value into the placeholder encapsulated with underscores.\n",
    "Put \"```\" as EOS token at the end of response.\n",
    "{}\n",
    "Do not capture any other values!\n",
    "If not specified, do not respond to that slot-value.\n",
    "\n",
    "Provide 1 posiible entity values based on the last utterance, along with their confidence (0-1). MAKE SURE TO SEPARATE EACH SLOT-VALUE WITH ITS CONFIDENCE PAIR.\n",
    "Format the output as:\n",
    "```json{{[{{\"state\": {{\"_entity1_\":\"_value1_\"}}, \"confidence\": \"X\"}}, {{\"state\": {{\"_entity2_\":\"_value2_\"}}, \"confidence\": \"X\"}}]}}```\n",
    "Where X is the Confidence of the answer.\n",
    "\n",
    "Now complete the following example, AND PROVIDE CONFIDENCE THAT IT'S CORRECT:\n",
    "input: {}  \n",
    "\n",
    "***Output JSON format***\n",
    "Output: ```json{{\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prompt5\n",
    "prompt = \"\"\"\n",
    "<|begin_of_text|>\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "Capture entity values from the LAST UTTERANCE of the conversation.\n",
    "FOCUS ONLY ON THE VALUES MENTIONED IN THE LAST UTTERANCE.\n",
    "Format the output as a valid JSON object for each entity-value pair.\n",
    "Format: {{\"state\": {{\"_entity_\":\"_value_\"}}}}\n",
    "Fill the actual entity value into the placeholder encapsulated with underscores.\n",
    "Put \"```\" as EOS token at the end of response.\n",
    "Values that should be captured are:\n",
    "{}\n",
    "Do not capture any other values!\n",
    "If not specified, do not respond to that slot-value.\n",
    "\n",
    "MAKE SURE TO SEPARATE EACH SLOT-VALUE PAIR.\n",
    "Format the output as:\n",
    "```json\n",
    "[\n",
    "  {{\"state\": {{\"_entity1_\": \"_value1_\"}}}},\n",
    "  {{\"state\": {{\"_entity2_\": \"_value2_\"}}}}\n",
    "]```\n",
    "\n",
    "Now complete the following example, AND PROVIDE CONFIDENCE THAT IT'S CORRECT:\n",
    "input: <|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "{}\n",
    "<|eot_id|>\n",
    "\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "***Output JSON format***\n",
    "Output: ```json\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_turn_info(dataset):\n",
    "    for dialog in dataset:\n",
    "        dialog_id = dialog[\"dialogue_id\"].split('.')[0].lower()\n",
    "        \n",
    "        last_state = {}\n",
    "        for tn in range(0, len(dialog[\"turns\"][\"utterance\"]), 2):\n",
    "            context = [f\"Customer: {t}\" if n % 2== 0 else f\"Assistant: {t}\" for n, t in enumerate(dialog[\"turns\"][\"utterance\"][:tn+1])]\n",
    "            state = dialog[\"turns\"][\"frames\"][tn][\"state\"]\n",
    "            \n",
    "            gt_domain = []\n",
    "            if len(state) == 0:\n",
    "                state = {}\n",
    "            else:\n",
    "                state = [state[i][\"slots_values\"] for i in range(len(state))]\n",
    "                state = [{k: v[0] for k, v in zip(state[i][\"slots_values_name\"], state[i][\"slots_values_list\"])} for i in range(len(state)) if len(state[i][\"slots_values_name\"]) > 0]\n",
    "            \n",
    "            new_state = last_state\n",
    "            for i in range(len(state)):\n",
    "                for sl, val in state[i].items():\n",
    "                    domain, name = sl.split(\"-\")\n",
    "                    if domain not in new_state:\n",
    "                        new_state[domain] = {name: val}\n",
    "                    else:\n",
    "                        new_state[domain][name] = val\n",
    "                        \n",
    "            state_update = {}\n",
    "            for domain, domain_state in new_state.items():\n",
    "                for slot, value in domain_state.items():\n",
    "                    if slot not in last_state.get(domain, {}) or last_state[domain][slot] != value:\n",
    "                        if domain not in gt_domain:\n",
    "                            print(f\"append domain: {domain}\")\n",
    "                            gt_domain.append(domain)\n",
    "                        if domain not in state_update:\n",
    "                            state_update[domain] = {}\n",
    "                        state_update[domain][slot] = value\n",
    "                        \n",
    "            last_state = new_state\n",
    "            \n",
    "            turn = {\n",
    "                \"question\":dialog[\"turns\"][\"utterance\"][tn],\n",
    "                \"gt_state\": last_state, # total state\n",
    "                \"dialog_id\": dialog_id,\n",
    "                \"metadata\": {\n",
    "                    \"domain\": gt_domain,\n",
    "                    \"turn_state\": state_update,\n",
    "                    \"total_state\": last_state,\n",
    "                    \"context\": \"\\n\".join(context[-6:])\n",
    "                }\n",
    "            }\n",
    "            yield turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from slot_description import DOMAIN_SLOT_DESCRIPTION, DOMAIN_EXPECTED_SLOT, EXPECTED_DOMAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_instruction_dataset(data_point):\n",
    "    ### INFO\n",
    "    gt_domain = data_point[\"metadata\"][\"domain\"]\n",
    "    context = data_point[\"metadata\"][\"context\"]\n",
    "    # print(\"context:\\n\")\n",
    "    # print(context)\n",
    "    utterance = data_point[\"question\"]\n",
    "    # print(\"utterance:\\n\")\n",
    "    # print(utterance)\n",
    "    turn_state = data_point[\"metadata\"][\"turn_state\"]\n",
    "    domain_description = \"\"\n",
    "    if gt_domain:\n",
    "        # print(f\"gt_domain: {gt_domain}\")\n",
    "        for domain in gt_domain:\n",
    "            domain_description += DOMAIN_SLOT_DESCRIPTION[domain]\n",
    "    \n",
    "    target_str = \"\"\n",
    "    for domain in turn_state.keys():\n",
    "        for slot, value in turn_state[domain].items():\n",
    "            buf = \"{\" + \"\\\"\" + \"state\\\": \" + \"{\\\"\" + str(slot) + \"\\\": \\\"\" + str(value) + \"\\\"}}, \"\n",
    "            # print(buf)\n",
    "            target_str += buf\n",
    "\n",
    "    if target_str.endswith(\", \"):\n",
    "        target_str = target_str[:-2]\n",
    "\n",
    "    target_str = \"[\" + target_str + \"]\" + \"```\" \n",
    "    text = \"###Prompt###\" + prompt.format(domain_description, context) + \"###Completion###\\n\" + target_str + tokenizer.eos_token\n",
    "    \n",
    "    return {\"text\": text, \"labels\": target_str}\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_dataset(data):\n",
    "    dataset = []\n",
    "    for i in range(len(data)):\n",
    "        dialog = data[i]\n",
    "        dataset.append(generate_instruction_dataset(dialog))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "def get_turn_info(dataset):\n",
    "    all_turns = []\n",
    "    for dialog in dataset:\n",
    "        dialog_id = dialog[\"dialogue_id\"].split('.')[0].lower()\n",
    "        \n",
    "        last_state = {}\n",
    "        for tn in range(0, len(dialog[\"turns\"][\"utterance\"]), 2):\n",
    "            context = [f\"Customer: {t}\" if n % 2== 0 else f\"Assistant: {t}\" for n, t in enumerate(dialog[\"turns\"][\"utterance\"][:tn+1])]\n",
    "            state = dialog[\"turns\"][\"frames\"][tn][\"state\"]\n",
    "            \n",
    "            gt_domain = []\n",
    "            if len(state) == 0:\n",
    "                state = {}\n",
    "            else:\n",
    "                state = [state[i][\"slots_values\"] for i in range(len(state))]\n",
    "                state = [{k: v[0] for k, v in zip(state[i][\"slots_values_name\"], state[i][\"slots_values_list\"])} for i in range(len(state)) if len(state[i][\"slots_values_name\"]) > 0]\n",
    "\n",
    "            new_state = copy.deepcopy(last_state)\n",
    "            for i in range(len(state)):\n",
    "                for sl, val in state[i].items():\n",
    "                    domain, name = sl.split(\"-\")\n",
    "                    if domain not in new_state:\n",
    "                        new_state[domain] = {name: val}\n",
    "                    else:\n",
    "                        new_state[domain][name] = val\n",
    "            state_update = {}\n",
    "            for domain, domain_state in new_state.items():\n",
    "                for slot, value in domain_state.items():\n",
    "                    if slot not in last_state.get(domain, {}) or last_state[domain][slot] != value:\n",
    "                        if domain not in state_update:\n",
    "                            state_update[domain] = {}\n",
    "                        state_update[domain][slot] = value\n",
    "                        \n",
    "            for domain, domain_state in state_update.items():\n",
    "                gt_domain.append(domain)\n",
    "            # if len(gt_domain) > 1:\n",
    "            #     print(f\"multiple gt_domain: {gt_domain}\")\n",
    "            last_state = new_state\n",
    "            \n",
    "            turn = {\n",
    "                \"question\":dialog[\"turns\"][\"utterance\"][tn],\n",
    "                \"gt_state\": copy.deepcopy(last_state), # total state\n",
    "                \"dialog_id\": copy.deepcopy(dialog_id),\n",
    "                \"metadata\": {\n",
    "                    \"domain\": copy.deepcopy(gt_domain),\n",
    "                    \"turn_state\": copy.deepcopy(state_update),\n",
    "                    \"total_state\": copy.deepcopy(last_state),\n",
    "                    \"context\": \"\\n\".join(context[-6:])\n",
    "                }\n",
    "            }\n",
    "            all_turns.append(turn)\n",
    "    \n",
    "    return all_turns\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_train_valid_data(sample):\n",
    "    dataset = load_dataset(\"multi_woz_v22\")\n",
    "\n",
    "    train_data = dataset[\"train\"]\n",
    "    valid_data = dataset[\"validation\"]\n",
    "    \n",
    "\n",
    "    if sample:\n",
    "        train_data = train_data.select([i for i in range(sample['train_size'])])\n",
    "        valid_data = valid_data.select([i for i in range(sample['valid_size'])])\n",
    "\n",
    "    train_turn_data = get_turn_info(train_data)\n",
    "    valid_turn_data = get_turn_info(valid_data)\n",
    "    \n",
    "    train_turn_data = process_dataset(train_turn_data)\n",
    "    valid_turn_data = process_dataset(valid_turn_data)\n",
    "    \n",
    "    return train_turn_data, valid_turn_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model_setup(model,batch_size,OUTPUT_DIR,epochs):\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=64,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], #specific to Llama models.\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    \n",
    "    model.gradient_checkpointing_enable()\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    \n",
    "    print(model)\n",
    "    \n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    training_arguments = TrainingArguments(\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=4,\n",
    "        optim=\"adamw_torch\",\n",
    "        logging_steps=1,\n",
    "        learning_rate=1e-6,\n",
    "        fp16=True,\n",
    "        max_grad_norm=0.3,\n",
    "        num_train_epochs=epochs,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=0.2,\n",
    "        warmup_ratio=0.05,\n",
    "        save_strategy=\"epoch\",\n",
    "        group_by_length=True,\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        report_to=\"wandb\",\n",
    "        save_safetensors=True,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        seed=42,\n",
    "    )\n",
    "    model.config.use_cache = True  # silence the warnings. Please re-enable for inference!\n",
    "    return lora_config,training_arguments,model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "batch_size = 5\n",
    "epochs = 1\n",
    "OUTPUT_DIR = \"llama3-confidence\"\n",
    "sample = None\n",
    "# sample = {\"train_size\" : 2000, \"valid_size\":400} # None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(model_id,stop_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/jenny0830/miniforge3/envs/llama/lib/python3.12/site-packages/datasets/load.py:1486: FutureWarning: The repository for multi_woz_v22 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/multi_woz_v22\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_data, validation_data = get_train_valid_data(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56776\n",
      "7374\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###Prompt###\n",
      "<|begin_of_text|>\n",
      "<|start_header_id|>system<|end_header_id|>\n",
      "Capture entity values from the LAST UTTERANCE of the conversation.\n",
      "FOCUS ONLY ON THE VALUES MENTIONED IN THE LAST UTTERANCE.\n",
      "Format the output as a valid JSON object for each entity-value pair.\n",
      "Format: {\"state\": {\"_entity_\":\"_value_\"}}\n",
      "Fill the actual entity value into the placeholder encapsulated with underscores.\n",
      "Put \"```\" as EOS token at the end of response.\n",
      "Values that should be captured are:\n",
      "\n",
      "\n",
      "In the DOMAIN of \"restaurant\", the values that should be captured are:\n",
      " - \"pricerange\" that specifies the price range of the restaurant (cheap/moderate/expensive)\n",
      " - \"area\" that specifies the area where the restaurant is located (north/east/west/south/centre)\n",
      " - \"food\" that specifies the type of food the restaurant serves\n",
      " - \"name\" that specifies the name of the restaurant\n",
      " - \"bookday\" that specifies the day of the booking\n",
      " - \"booktime\" that specifies the time of the booking\n",
      " - \"bookpeople\" that specifies for how many people is the booking made\n",
      "\n",
      "Do not capture any other values!\n",
      "If not specified, do not respond to that slot-value.\n",
      "\n",
      "MAKE SURE TO SEPARATE EACH SLOT-VALUE PAIR.\n",
      "Format the output as:\n",
      "```json\n",
      "[\n",
      "  {\"state\": {\"_entity1_\": \"_value1_\"}},\n",
      "  {\"state\": {\"_entity2_\": \"_value2_\"}}\n",
      "]```\n",
      "\n",
      "Now complete the following example, AND PROVIDE CONFIDENCE THAT IT'S CORRECT:\n",
      "input: <|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "Customer: i need a place to dine in the center thats expensive\n",
      "<|eot_id|>\n",
      "\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "***Output JSON format***\n",
      "Output: ```json\n",
      "###Completion###\n",
      "[{\"state\": {\"area\": \"centre\"}}, {\"state\": {\"pricerange\": \"expensive\"}}]```<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###Prompt###\n",
      "<|begin_of_text|>\n",
      "<|start_header_id|>system<|end_header_id|>\n",
      "Capture entity values from the LAST UTTERANCE of the conversation.\n",
      "FOCUS ONLY ON THE VALUES MENTIONED IN THE LAST UTTERANCE.\n",
      "Format the output as a valid JSON object for each entity-value pair.\n",
      "Format: {\"state\": {\"_entity_\":\"_value_\"}}\n",
      "Fill the actual entity value into the placeholder encapsulated with underscores.\n",
      "Put \"```\" as EOS token at the end of response.\n",
      "Values that should be captured are:\n",
      "\n",
      "\n",
      "In the DOMAIN of \"restaurant\", the values that should be captured are:\n",
      " - \"pricerange\" that specifies the price range of the restaurant (cheap/moderate/expensive)\n",
      " - \"area\" that specifies the area where the restaurant is located (north/east/west/south/centre)\n",
      " - \"food\" that specifies the type of food the restaurant serves\n",
      " - \"name\" that specifies the name of the restaurant\n",
      " - \"bookday\" that specifies the day of the booking\n",
      " - \"booktime\" that specifies the time of the booking\n",
      " - \"bookpeople\" that specifies for how many people is the booking made\n",
      "\n",
      "Do not capture any other values!\n",
      "If not specified, do not respond to that slot-value.\n",
      "\n",
      "MAKE SURE TO SEPARATE EACH SLOT-VALUE PAIR.\n",
      "Format the output as:\n",
      "```json\n",
      "[\n",
      "  {\"state\": {\"_entity1_\": \"_value1_\"}},\n",
      "  {\"state\": {\"_entity2_\": \"_value2_\"}}\n",
      "]```\n",
      "\n",
      "Now complete the following example, AND PROVIDE CONFIDENCE THAT IT'S CORRECT:\n",
      "input: <|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "Customer: I'm looking for a local place to dine in the centre that serves chinese food.\n",
      "<|eot_id|>\n",
      "\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "***Output JSON format***\n",
      "Output: ```json\n",
      "###Completion###\n",
      "[{\"state\": {\"area\": \"centre\"}}, {\"state\": {\"food\": \"chinese\"}}]```<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(validation_data[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['bnb_8bit_use_double_quant', 'bnb_8bit_quant_type', 'bnb_8bit_compute_dtype']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aea7e17df804393b4dda40e12497000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_loaded = \"false\"\n",
    "if model_loaded == \"false\":\n",
    "    model = get_model(model_id,is_8bit = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lora_config,training_arguments,model = get_model_setup(model,batch_size,OUTPUT_DIR,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "class CustomWandbLoggingCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, **kwargs):\n",
    "        if state.is_world_process_zero:\n",
    "            logs = {k: v for k, v in state.log_history[-1].items() if isinstance(v, (int, float))}\n",
    "            wandb.log(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fine_tune_model(train_subset, valid_subset, saved_model):\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_subset,\n",
    "        eval_dataset=valid_subset,\n",
    "        peft_config=lora_config,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=1500,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_arguments,\n",
    "        callbacks=[CustomWandbLoggingCallback()],\n",
    "    )\n",
    "    trainer.train()\n",
    "    \n",
    "    trainer.model.save_pretrained(saved_model)\n",
    "    tokenizer.save_pretrained(saved_model)\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_model(saved_model):\n",
    "    model = AutoModelForCausalLM.from_pretrained(saved_model, cache_dir=\"cache\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(saved_model, cache_dir=\"cache\")\n",
    "    \n",
    "    command = [\n",
    "        \"python\", \"run.py\",\n",
    "        \"--model_name=saved_model\",\n",
    "        \"--dials_total=1\",\n",
    "        \"--temperature=1\"\n",
    "    ]\n",
    "    \n",
    "    result = subprocess.run(command, capture_output=True, text=True)\n",
    "    print(f\"saved_model: {saved_model}\")\n",
    "    print(f\"result: {result}\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjennysun-cs09\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/bcms/jenny0830/Prompting-Strategies/wandb/run-20240602_155200-mim3kn8g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jennysun-cs09/llama3-finetuning-DST/runs/mim3kn8g' target=\"_blank\">fullset_epoch1_lr6</a></strong> to <a href='https://wandb.ai/jennysun-cs09/llama3-finetuning-DST' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jennysun-cs09/llama3-finetuning-DST' target=\"_blank\">https://wandb.ai/jennysun-cs09/llama3-finetuning-DST</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jennysun-cs09/llama3-finetuning-DST/runs/mim3kn8g' target=\"_blank\">https://wandb.ai/jennysun-cs09/llama3-finetuning-DST/runs/mim3kn8g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f01d99636394a2fb73dd1f20319e532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/56776 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "780eb28690ef4f7fb246f3776479659b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7374 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/u/jenny0830/miniforge3/envs/llama/lib/python3.12/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/u/jenny0830/miniforge3/envs/llama/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2839' max='2839' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2839/2839 9:09:40, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>568</td>\n",
       "      <td>0.800600</td>\n",
       "      <td>0.838747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1136</td>\n",
       "      <td>0.815500</td>\n",
       "      <td>0.594729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1704</td>\n",
       "      <td>0.519800</td>\n",
       "      <td>0.533505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2272</td>\n",
       "      <td>0.454800</td>\n",
       "      <td>0.504787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.007 MB of 0.007 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>eval/loss</td><td>█▃▂▁</td></tr><tr><td>eval/runtime</td><td>█▁▆▃</td></tr><tr><td>eval/samples_per_second</td><td>▁█▂▆</td></tr><tr><td>eval/steps_per_second</td><td>▁█▃▆</td></tr><tr><td>eval_loss</td><td>█▃▂▁</td></tr><tr><td>eval_runtime</td><td>█▁▆▃</td></tr><tr><td>eval_samples_per_second</td><td>▁█▂▆</td></tr><tr><td>eval_steps_per_second</td><td>▁█▃▆</td></tr><tr><td>grad_norm</td><td>█▇▂▂▃▅▅▂▂▂▂▄▂▂▁▂▃▁▁▁▁▂▃▁▂▁▁▂▁▁▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>learning_rate</td><td>▃▆███████▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>██▆▅▅▄▂▂▂▂▂▁▂▂▂▂▂▂▂▁▂▂▁▂▁▂▂▂▂▂▂▂▂▁▂▁▂▂▁▂</td></tr><tr><td>step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>total_flos</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>█▇▂▂▃▅▅▂▂▂▂▄▂▂▁▂▃▁▁▁▁▂▃▁▂▁▁▂▁▁▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▃▆███████▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>██▆▅▅▄▂▂▂▂▂▁▂▂▂▂▂▂▂▁▂▂▁▂▁▂▂▂▂▂▂▂▂▁▂▁▂▂▁▂</td></tr><tr><td>train_loss</td><td>▁</td></tr><tr><td>train_runtime</td><td>▁</td></tr><tr><td>train_samples_per_second</td><td>▁</td></tr><tr><td>train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1.0</td></tr><tr><td>eval/loss</td><td>0.50479</td></tr><tr><td>eval/runtime</td><td>627.6184</td></tr><tr><td>eval/samples_per_second</td><td>11.749</td></tr><tr><td>eval/steps_per_second</td><td>1.469</td></tr><tr><td>eval_loss</td><td>0.50479</td></tr><tr><td>eval_runtime</td><td>627.6184</td></tr><tr><td>eval_samples_per_second</td><td>11.749</td></tr><tr><td>eval_steps_per_second</td><td>1.469</td></tr><tr><td>grad_norm</td><td>5.02621</td></tr><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>loss</td><td>0.1545</td></tr><tr><td>step</td><td>2839</td></tr><tr><td>total_flos</td><td>1.0608649509741773e+18</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>2839</td></tr><tr><td>train/grad_norm</td><td>5.02621</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.1545</td></tr><tr><td>train_loss</td><td>0.83491</td></tr><tr><td>train_runtime</td><td>33014.7956</td></tr><tr><td>train_samples_per_second</td><td>1.72</td></tr><tr><td>train_steps_per_second</td><td>0.086</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fullset_epoch1_lr6</strong> at: <a href='https://wandb.ai/jennysun-cs09/llama3-finetuning-DST/runs/mim3kn8g' target=\"_blank\">https://wandb.ai/jennysun-cs09/llama3-finetuning-DST/runs/mim3kn8g</a><br/> View project at: <a href='https://wandb.ai/jennysun-cs09/llama3-finetuning-DST' target=\"_blank\">https://wandb.ai/jennysun-cs09/llama3-finetuning-DST</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240602_155200-mim3kn8g/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"llama3-finetuning-DST\", entity=\"jennysun-cs09\", name=\"fullset_epoch1_lr6\")\n",
    "wandb.config.update(training_arguments)\n",
    "\n",
    "train_subset = Dataset.from_dict({key: [dic[key] for dic in train_data] for key in train_data[0]})\n",
    "valid_subset = Dataset.from_dict({key: [dic[key] for dic in validation_data] for key in validation_data[0]})\n",
    "\n",
    "saved_model = os.path.join(\"saved_model_5\", f\"fullset_epoch1_lr6\")\n",
    "model = fine_tune_model(train_subset, valid_subset, saved_model)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 250 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_subset = train_data[:train_samples]\n",
    "print(f\"train_subset len: {len(train_subset)}\")\n",
    "train_data = train_data[train_samples:]\n",
    "print(f\"train_data len: {len(train_data)}\")\n",
    "\n",
    "train_subset = Dataset.from_dict({key: [dic[key] for dic in train_data] for key in train_subset[0]})\n",
    "valid_subset = Dataset.from_dict({key: [dic[key] for dic in validation_data[:valid_samples]] for key in validation_data[0]})\n",
    "print(f\"valid_subset len: {len(valid_subset)}\")\n",
    "saved_model = os.path.join(\"saved_model_5\", f\"250_sample\")\n",
    "model = fine_tune_model(train_subset, valid_subset, saved_model)\n",
    "\n",
    "# result = evaluate_model(saved_model)\n",
    "# JGA.append(result[\"JGA\"])\n",
    "# ECE.append(result[\"ECE\"])\n",
    "# AUC.append(result[\"AUC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "saved_model = os.path.join(\"saved_model_2\", f\"250_sample\")\n",
    "model = AutoModelForCausalLM.from_pretrained(saved_model, cache_dir=\"cache\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_model, cache_dir=\"cache\")\n",
    "!python run.py --model_name=\"saved_model\" --dials_total=100 --temperature=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://wandb.ai/jennysun-cs09/prompting-strategies/runs/czxogh1t\n",
    "JGA.append(0.2468)\n",
    "ECE.append(0.2366)\n",
    "AUC.append(0.8219)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_subset = train_data[:train_samples]\n",
    "print(f\"train_subset len: {len(train_subset)}\")\n",
    "train_data = train_data[train_samples:]\n",
    "print(f\"train_data len: {len(train_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 500 Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "saved_model = os.path.join(\"saved_model\", f\"250_sample\")\n",
    "model = AutoModelForCausalLM.from_pretrained(saved_model, cache_dir=\"cache\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_model, cache_dir=\"cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_subset = train_data[:train_samples]\n",
    "print(f\"train_subset len: {len(train_subset)}\")\n",
    "train_data = train_data[train_samples:]\n",
    "print(f\"train_data len: {len(train_data)}\")\n",
    "\n",
    "train_subset = Dataset.from_dict({key: [dic[key] for dic in train_data] for key in train_subset[0]})\n",
    "valid_subset = Dataset.from_dict({key: [dic[key] for dic in validation_data[:valid_samples]] for key in validation_data[0]})\n",
    "print(f\"valid_subset len: {len(valid_subset)}\")\n",
    "saved_model = os.path.join(\"saved_model\", f\"500_sample\")\n",
    "model = fine_tune_model(train_subset, valid_subset, saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "saved_model = os.path.join(\"saved_model\", f\"500_sample\")\n",
    "model = AutoModelForCausalLM.from_pretrained(saved_model, cache_dir=\"cache\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_model, cache_dir=\"cache\")\n",
    "!python run.py --model_name=\"saved_model\" --dials_total=100 --temperature=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://wandb.ai/jennysun-cs09/prompting-strategies/runs/82ksgaq2\n",
    "JGA.append(0.2624)\n",
    "ECE.append(0.2328)\n",
    "AUC.append(0.8223)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 750 Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = os.path.join(\"saved_model\", f\"500_sample\")\n",
    "model = AutoModelForCausalLM.from_pretrained(saved_model, cache_dir=\"cache\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_model, cache_dir=\"cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset = train_data[:train_samples]\n",
    "print(f\"train_subset len: {len(train_subset)}\")\n",
    "train_data = train_data[train_samples:]\n",
    "print(f\"train_data len: {len(train_data)}\")\n",
    "\n",
    "train_subset = Dataset.from_dict({key: [dic[key] for dic in train_data] for key in train_subset[0]})\n",
    "valid_subset = Dataset.from_dict({key: [dic[key] for dic in validation_data[:valid_samples]] for key in validation_data[0]})\n",
    "print(f\"valid_subset len: {len(valid_subset)}\")\n",
    "saved_model = os.path.join(\"saved_model\", f\"750_sample\")\n",
    "model = fine_tune_model(train_subset, valid_subset, saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = os.path.join(\"saved_model\", f\"750_sample\")\n",
    "model = AutoModelForCausalLM.from_pretrained(saved_model, cache_dir=\"cache\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_model, cache_dir=\"cache\")\n",
    "!python run.py --model_name=\"saved_model\" --dials_total=100 --temperature=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://wandb.ai/jennysun-cs09/prompting-strategies/runs/l78q0sgn\n",
    "JGA.append(0.2560)\n",
    "ECE.append(0.2212)\n",
    "AUC.append(0.8173)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 1000 Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = os.path.join(\"saved_model\", f\"750_sample\")\n",
    "model = AutoModelForCausalLM.from_pretrained(saved_model, cache_dir=\"cache\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_model, cache_dir=\"cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset = train_data[:train_samples]\n",
    "print(f\"train_subset len: {len(train_subset)}\")\n",
    "train_data = train_data[train_samples:]\n",
    "print(f\"train_data len: {len(train_data)}\")\n",
    "\n",
    "train_subset = Dataset.from_dict({key: [dic[key] for dic in train_data] for key in train_subset[0]})\n",
    "valid_subset = Dataset.from_dict({key: [dic[key] for dic in validation_data[:valid_samples]] for key in validation_data[0]})\n",
    "print(f\"valid_subset len: {len(valid_subset)}\")\n",
    "saved_model = os.path.join(\"saved_model\", f\"1000_sample\")\n",
    "model = fine_tune_model(train_subset, valid_subset, saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = os.path.join(\"saved_model\", f\"1000_sample\")\n",
    "model = AutoModelForCausalLM.from_pretrained(saved_model, cache_dir=\"cache\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_model, cache_dir=\"cache\")\n",
    "!python run.py --model_name=\"saved_model\" --dials_total=100 --temperature=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://wandb.ai/jennysun-cs09/prompting-strategies/runs/o2ok12o3\n",
    "JGA.append(0.2751)\n",
    "ECE.append(0.2136)\n",
    "AUC.append(0.8114)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 1250 Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = os.path.join(\"saved_model\", f\"1000_sample\")\n",
    "model = AutoModelForCausalLM.from_pretrained(saved_model, cache_dir=\"cache\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_model, cache_dir=\"cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset = train_data[:train_samples]\n",
    "print(f\"train_subset len: {len(train_subset)}\")\n",
    "train_data = train_data[train_samples:]\n",
    "print(f\"train_data len: {len(train_data)}\")\n",
    "\n",
    "train_subset = Dataset.from_dict({key: [dic[key] for dic in train_data] for key in train_subset[0]})\n",
    "valid_subset = Dataset.from_dict({key: [dic[key] for dic in validation_data[:valid_samples]] for key in validation_data[0]})\n",
    "print(f\"valid_subset len: {len(valid_subset)}\")\n",
    "saved_model = os.path.join(\"saved_model\", f\"1250_sample\")\n",
    "model = fine_tune_model(train_subset, valid_subset, saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = os.path.join(\"saved_model\", f\"1250_sample\")\n",
    "model = AutoModelForCausalLM.from_pretrained(saved_model, cache_dir=\"cache\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_model, cache_dir=\"cache\")\n",
    "!python run.py --model_name=\"saved_model\" --dials_total=100 --temperature=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://wandb.ai/jennysun-cs09/prompting-strategies/runs/bn6z99b0\n",
    "JGA.append(0.2624)\n",
    "ECE.append(0.2328)\n",
    "AUC.append(0.8223)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 1500 Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = os.path.join(\"saved_model\", f\"1250_sample\")\n",
    "model = AutoModelForCausalLM.from_pretrained(saved_model, cache_dir=\"cache\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_model, cache_dir=\"cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset = train_data[:train_samples]\n",
    "print(f\"train_subset len: {len(train_subset)}\")\n",
    "train_data = train_data[train_samples:]\n",
    "print(f\"train_data len: {len(train_data)}\")\n",
    "\n",
    "train_subset = Dataset.from_dict({key: [dic[key] for dic in train_data] for key in train_subset[0]})\n",
    "valid_subset = Dataset.from_dict({key: [dic[key] for dic in validation_data[:valid_samples]] for key in validation_data[0]})\n",
    "print(f\"valid_subset len: {len(valid_subset)}\")\n",
    "saved_model = os.path.join(\"saved_model\", f\"1500_sample\")\n",
    "model = fine_tune_model(train_subset, valid_subset, saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = os.path.join(\"saved_model\", f\"1500_sample\")\n",
    "model = AutoModelForCausalLM.from_pretrained(saved_model, cache_dir=\"cache\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_model, cache_dir=\"cache\")\n",
    "!python run.py --model_name=\"saved_model\" --dials_total=100 --temperature=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1750 Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = os.path.join(\"saved_model\", f\"1500_sample\")\n",
    "model = AutoModelForCausalLM.from_pretrained(saved_model, cache_dir=\"cache\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_model, cache_dir=\"cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset = train_data[:train_samples]\n",
    "print(f\"train_subset len: {len(train_subset)}\")\n",
    "train_data = train_data[train_samples:]\n",
    "print(f\"train_data len: {len(train_data)}\")\n",
    "\n",
    "train_subset = Dataset.from_dict({key: [dic[key] for dic in train_data] for key in train_subset[0]})\n",
    "valid_subset = Dataset.from_dict({key: [dic[key] for dic in validation_data[:valid_samples]] for key in validation_data[0]})\n",
    "print(f\"valid_subset len: {len(valid_subset)}\")\n",
    "saved_model = os.path.join(\"saved_model\", f\"1750_sample\")\n",
    "model = fine_tune_model(train_subset, valid_subset, saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = os.path.join(\"saved_model\", f\"1750_sample\")\n",
    "model = AutoModelForCausalLM.from_pretrained(saved_model, cache_dir=\"cache\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_model, cache_dir=\"cache\")\n",
    "!python run.py --model_name=\"saved_model\" --dials_total=100 --temperature=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2000 Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = os.path.join(\"saved_model\", f\"1750_sample\")\n",
    "model = AutoModelForCausalLM.from_pretrained(saved_model, cache_dir=\"cache\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_model, cache_dir=\"cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset = train_data[:train_samples]\n",
    "print(f\"train_subset len: {len(train_subset)}\")\n",
    "train_data = train_data[train_samples:]\n",
    "print(f\"train_data len: {len(train_data)}\")\n",
    "\n",
    "train_subset = Dataset.from_dict({key: [dic[key] for dic in train_data] for key in train_subset[0]})\n",
    "valid_subset = Dataset.from_dict({key: [dic[key] for dic in validation_data[:valid_samples]] for key in validation_data[0]})\n",
    "print(f\"valid_subset len: {len(valid_subset)}\")\n",
    "saved_model = os.path.join(\"saved_model\", f\"2000_sample\")\n",
    "model = fine_tune_model(train_subset, valid_subset, saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = os.path.join(\"saved_model\", f\"2000_sample\")\n",
    "model = AutoModelForCausalLM.from_pretrained(saved_model, cache_dir=\"cache\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_model, cache_dir=\"cache\")\n",
    "!python run.py --model_name=\"saved_model\" --dials_total=100 --temperature=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(training_size, JGA, label=\"JGA\")\n",
    "plt.xlabel(\"Number of Training Samples (Turn)\")\n",
    "plt.ylabel(\"JGA\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(training_size, ECE, label=\"ECE\")\n",
    "plt.xlabel(\"Number of Training Samples (Turn)\")\n",
    "plt.ylabel(\"ECE\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(training_size, AUC, label=\"AUC\")\n",
    "plt.xlabel(\"Number of Training Samples (Turn)\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_path = \"saved_model\"\n",
    "model = AutoModelForCausalLM.from_pretrained(save_path, cache_dir=\"cache\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_path, cache_dir=\"cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python run.py --model_name=\"saved_model\" --dials_total=100 --temperature=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a custom stopping criteria class for the model's text generation.\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList, TextIteratorStreamer\n",
    "\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        stop_ids = [74694,55375,5658,14196]  # IDs of tokens where the generation should stop.\n",
    "        for stop_id in stop_ids:\n",
    "            if input_ids[0][-1] == stop_id:  # Checking if the last generated token is a stop token.\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stop = StopOnTokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer = TextIteratorStreamer(tokenizer, timeout=10., skip_prompt=True, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index = 10\n",
    "input_text = validation_data[data_index]['text']\n",
    "labels = validation_data[data_index]['labels']\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").input_ids.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "    inputs,\n",
    "    streamer=streamer,\n",
    "    max_new_tokens=200,  # Adjust max_length as needed\n",
    "    early_stopping=True,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    temperature=0.7,\n",
    "    repetition_penalty=1.0,\n",
    "    num_beams=1,\n",
    "    stopping_criteria=StoppingCriteriaList([stop])\n",
    ")\n",
    "output = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "print(\"### output #### : \", output.strip())\n",
    "print(\"### labels #### : \", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
