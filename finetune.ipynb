{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Fine-Tuning Llama3 with Quantization and LoRA\n",
    "\n",
    "Hello and welcome to this tutorial on fine-tuning the Llama3 model!\n",
    "Whether you're new to model training or looking to refine your skills, this guide will walk you through the steps to customize a pre-trained model to better suit your specific need.\n",
    "\n",
    "In this tutorial, we'll learn how to fine-tune the Llama3 model using quantizaiton techniques and Low-Rank Adaptation (LoRA). By the end of this guide, you will be able to adjust a pre-trained model to perform better on your specific tasks or datasets.\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tunine\n",
    "### What is Fine-tuning?\n",
    "Fine-tuning is the process of refining a pre-trained model to improve its performance on a specific task or dataset. It involves:\n",
    "- Adjusting the model's weights and biases\n",
    "- Adapting the model to understand and work with new data\n",
    "- Customizing a general-purpose model trained on large datsets to be more effective for smaller, specialized datasets\n",
    "\n",
    "### Why is Fine-tuning important?\n",
    "Fine-tuning is crucial because it allows us to:\n",
    "- Utilize the capabilities of large, pre-trained models for specific applications\n",
    "- Achieve high performance on specialized tasks without needing to train a model from scratch\n",
    "- Save computational resources and time, especially when large datasets or extensive computational power is not available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Setup\n",
    "In this section, I will describe the GPU resources allocated for the project, list the required packages in __'requirements.txt'__, and provide instructions on how to create and manage the Conda environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Resources\n",
    "For this project, we have allocated GPU resources from the NCSA Delta systems. Here are the details of the GPU resources used:\n",
    "* GPU Type: NVIDIA A100x4\n",
    "* Memory: 40960 MiB\n",
    "\n",
    "You can obtain detailed GPU usage informaiton by running the __'nvidai-msi'__ command. Below is an example of the GPU usage during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "+-----------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.183.01    Driver Version: 535.183.01    CUDA Version: 12.2   |\n",
    "|-------------------------------+----------------------+----------------------+\n",
    "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                               |                      |               MIG M. |\n",
    "|===============================+======================+======================|\n",
    "|   0  NVIDIA A100-SXM4-40GB On | 00000000:C7:00.0 Off |                    0 |\n",
    "| N/A   49C    P0    341W / 400W |  25284MiB / 40960MiB |     97%      Default |\n",
    "|                               |                      |               Disabled |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "\n",
    "+-----------------------------------------------------------------------------+\n",
    "| Processes:                                                                  |\n",
    "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
    "|        ID   ID                                                   Usage      |\n",
    "|=============================================================================|\n",
    "|    0   N/A  N/A     184712      C   ...nvs/llama-new/bin/python   25274MiB |\n",
    "+-----------------------------------------------------------------------------+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "To ensure all dependencies are correctly installed, we will create a Conda environment and install the required packages from the __'requirement.txt'__ file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step1: Create and Activate Conda Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a new Conda environment:\n",
    "Replace __'llama3_finetuning'__ with your desired environment name and __'python=3.9'__ with your preferred Python version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "conda create -n llama3_finetuning python=3.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Activate the Conda environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "conda activate llama3_finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step2: Install Required Packages\n",
    "Install the required packages from the __'requirements.txt'__ file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "### Verifying the Setup\n",
    "To verify that all packages are installed correctly, you can list the installed packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "This command will display all the packages and their versions installed in your current Conda environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory\n",
    "When training a model, the following hyperparameters can impact mamory usage and training time\n",
    "1. Batch size\n",
    "- Description: The number of training samples processed in one forward/ backward pass\n",
    "- Impact on Memory: Larger batch sizes require more memory because the GPU needs to store data for all samples in the batch\n",
    "\n",
    "2. Epochs\n",
    "- Description: The number of times the entire training dataset is passed through the model\n",
    "- Impact on Running time: The number of epochs does not significantly affect the peak memory usage during each forward and backward pass. However, more epochs will increase the total training time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Errors\n",
    "If you encounter following errors, here are some solutions\n",
    "\n",
    "![Error 1](error1.png)\n",
    "\n",
    "Restart your session and allocate a new GPU from the Delta systems\n",
    "\n",
    "![Error 2](error2.png)\n",
    "\n",
    "Reduce the batch_size hyperparameter. If the error persists then restart your session and allocate a new GPU from the Delta system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Code Explanation\n",
    "In this section, we will walk through the fine-tuning code. The code is structured to handle model quantization, apply Low-Rank Adaptation (LoRA), and set up training arguments. Additionally, we will include custom logging with Weights & Biases (wandb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization\n",
    "Quantization reduces the precision of the numbers used to represent the model parameters, thereby reducing memory usage and speeding up computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __'get_model'__ function is responsible for loading the pre-trained model with either 8-bit or 4-bit quantization based on the __'is_8bit'__ flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "from transformers import ( \n",
    "    AutoModelForCausalLM, \n",
    "    BitsAndBytesConfig,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    ")\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "os.environ['HF_TOKEN'] = 'your won token'\n",
    "os.environ[\"AZURE_OPENAI_KEY\"] = \"your own token\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model(model_id, is_8bit=True):\n",
    "    if is_8bit:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            bnb_8bit_use_double_quant=True,\n",
    "            bnb_8bit_quant_type=\"nf4\",\n",
    "            bnb_8bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "    else:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        cache_dir=\"cache\"\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model_id = 'meta-llama/Meta-Llama-3-8B-Instruct'\n",
    "model = get_model(model_id, is_8bit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response Generation with Stopping Criteria\n",
    "This section of the code defines how to generate responses from a model with specific stopping criteria In includes the implementation of a custom stopping criterion and a function to generate responses using either Azure OPenAI service or a local model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        stop_ids = [74694,55375,5658,14196]  # IDs of tokens where the generation should stop.\n",
    "        for stop_id in stop_ids:\n",
    "            if input_ids[0][-1] == stop_id:  # Checking if the last generated token is a stop token.\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def response(model, model_id, streamer, model_inputs, temperature, device=\"cuda\"): \n",
    "    stop = StopOnTokens()\n",
    "    model_inputs = model_inputs.to(next(model.parameters()).device)\n",
    "    if model_id == \"UIUC-ConvAI-Sweden-GPT4\":\n",
    "        client = AzureOpenAI(\n",
    "            azure_endpoint = \"https://uiuc-convai-sweden.openai.azure.com/\", \n",
    "            api_key=os.getenv(\"AZURE_OPENAI_KEY\"),  \n",
    "            api_version=\"2024-02-15-preview\"\n",
    "            )\n",
    "        completion = client.chat.completions.create(\n",
    "                        model=\"UIUC-ConvAI-Sweden-GPT4\", # model = \"deployment_name\"\n",
    "                        messages = [{\"role\": \"user\", \"content\": model_inputs}],\n",
    "                        temperature=temperature,\n",
    "                        max_tokens=200,\n",
    "                        top_p=0.95,\n",
    "                        frequency_penalty=0,\n",
    "                        presence_penalty=0,\n",
    "                        stop=None\n",
    "                        )\n",
    "        outputs = completion.choices[0].message.content\n",
    "    else:\n",
    "        outputs = model.generate(\n",
    "            input_ids=model_inputs,\n",
    "            streamer=streamer,\n",
    "            max_new_tokens=200,\n",
    "            early_stopping=True,\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            top_k=50,\n",
    "            temperature=temperature,\n",
    "            repetition_penalty=1.0,\n",
    "            num_beams=1,\n",
    "            output_scores=True, \n",
    "            return_dict_in_generate=True,\n",
    "            stopping_criteria=StoppingCriteriaList([stop]),\n",
    "        )\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter-Efficient Fine-Tuning (PEFT)\n",
    "Parameter-Efficient Fine-Tuning (PEFT) referes to a collection of techniques aimed at fine-tuning large pre-trained models by adjusting only a small subset of their parameters. The primary goal of PEFT is to adpat models to new tasks efficiently, reducing computational costs and memory usage while maintaining high performance. PEFT is particularly useful when dealing with very large models that would be otherwise prohibitively expensive to fine-tune in their entirety.\n",
    "\n",
    "### Low-Rank Adaptation (LoRA)\n",
    "LoRA is a sppecific type of PEFT. It helps in reducing the number of trainable parameters, making the fine-tuning process more efficient. By inserting low-rank matrices into the model, LoRA allows for significant memory and computational saving while still enabling the model to adapt effectively to new tasks\n",
    "\n",
    "#### LoRA Configuration Parameters:\n",
    "* __r__ -  The rank of the low-rank matrices used in LoRA, A lower rank means fewer parameters.\n",
    "* __lora_alpha__ - Scaling factor for the low-rank matrices. A higher alpha can help maintain the expressiveness of the model despite the low-rank approximation\n",
    "* __target_modules__ - The specific moduls of the model where LoRA is applied. For llama models, this includes the query (q_proj), key (k_proj), value (v_proj), and output (o_proj) projection layers.\n",
    "* __lora_dropout__ - The dropout rate applied to the LoRA layers, helping to prevent overfitting\n",
    "* __bias__ -  Whether to train the bias terms in the layers where LoRA is applied\n",
    "* __task_type__ - The type of task for which the model is being fine-tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __'get_model_setup'__ function applies LoRA to the model and configures it for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "def get_model_setup(model, batch_size, OUTPUT_DIR, epochs):\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=64,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # specific to Llama models\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    \n",
    "    model.gradient_checkpointing_enable()\n",
    "    # Prepare model for k-bit training\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    # Apply LoRA to the model\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    training_arguments = TrainingArguments(\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=4,\n",
    "        optim=\"adamw_torch\",\n",
    "        logging_steps=1,\n",
    "        learning_rate=1e-4,\n",
    "        fp16=True,\n",
    "        max_grad_norm=0.3,\n",
    "        num_train_epochs=epochs,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=0.2,\n",
    "        warmup_ratio=0.05,\n",
    "        save_strategy=\"epoch\",\n",
    "        group_by_length=True,\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        report_to=\"wandb\",\n",
    "        save_safetensors=True,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        seed=42,\n",
    "    )\n",
    "    model.config.use_cache = True  # silence the warnings. Please re-enable for inference!\n",
    "    return lora_config, training_arguments, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hyperparameters setting\n",
    "batch_size = 5\n",
    "OUTPUT_DIR = \"outputs\"\n",
    "epochs = 1\n",
    "\n",
    "lora_config, training_arguments, model = get_model_setup(model, batch_size, OUTPUT_DIR, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer Setup\n",
    "The tokenizer converts input text into toekn IDs that can be processed by the model. It also handles padding and truncation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __'get_tokenizer'__ function loads the tokenizer and sets the padding token to be the end-of-sequence token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TextIteratorStreamer\n",
    "\n",
    "def get_tokenizer(model_id, stop_tokens=True):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id  # Set padding token to EOS token\n",
    "    return tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __'TextIteratorStreamer'__ is used to handle the token generation process efficiently, especially when working with LLMs. It allos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "streamer = TextIteratorStreamer(\n",
    "    tokenizer, \n",
    "    timeout = 10, \n",
    "    skip_prompt = True, \n",
    "    skip_special_tokens = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Logging with Weights & Biases (wandb)\n",
    "To monitor and log the training process, we integrate wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class CustomWandbLoggingCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, **kwargs):\n",
    "        if state.is_world_process_zero:\n",
    "            logs = {k: v for k, v in state.log_history[-1].items() if isinstance(v, (int, float))}\n",
    "            wandb.log(logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning the Model\n",
    "This section describes how to set up and run the fine-tuning process using the SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SFTTrainer\n",
    "The __'SFTTrainer'__ (Supervised Fine-Tuning Trainer) is a specialized training class from the __'trl'__ library designed for fine-tuning LM with supervised data. It extends the Hugging Face __'Trainer'__ class, providing additional functionalities tailored for fine-tuning tasks, such as managing parameter-efficient fine-tuning methods like LoRA, handling large datasets, and integrating custom callbacks for logging and monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "In this section, we introduce the basic idea of fine-tuning the model to generate responses that include verbalized confidence scores. The goal is to improve the model's ability to predict dialogue states and verbalize its confidence accurately, enhancing overall performance and reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "<|begin_of_text|>\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "Capture entity values from the LAST UTTERANCE of the conversation.\n",
    "FOCUS ONLY ON THE VALUES MENTIONED IN THE LAST UTTERANCE.\n",
    "Format the output as a valid JSON object for each entity-value pair.\n",
    "Format: {{\"state\": {{\"_entity_\":\"_value_\"}}}}\n",
    "Fill the actual entity value into the placeholder encapsulated with underscores.\n",
    "Put \"```\" as EOS token at the end of response.\n",
    "Values that should be captured are:\n",
    "{}\n",
    "Do not capture any other values!\n",
    "If not specified, do not respond to that slot-value.\n",
    "\n",
    "MAKE SURE TO SEPARATE EACH SLOT-VALUE PAIR.\n",
    "Format the output as:\n",
    "```json\n",
    "[\n",
    "  {{\"state\": {{\"_entity1_\": \"_value1_\"}}}},\n",
    "  {{\"state\": {{\"_entity2_\": \"_value2_\"}}}}\n",
    "]```\n",
    "\n",
    "Now complete the following example, AND PROVIDE CONFIDENCE THAT IT'S CORRECT:\n",
    "input: <|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "{}\n",
    "<|eot_id|>\n",
    "\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "***Output JSON format***\n",
    "Output: ```json\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_confidence = \"\"\"\n",
    "<|begin_of_text|>\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "Capture entity values from the LAST UTTERANCE of the conversation.\n",
    "FOCUS ONLY ON THE VALUES MENTIONED IN THE LAST UTTERANCE.\n",
    "Format the output as a valid JSON object, and for each entity-value pair, along with their pair-level confidence (0-1).\n",
    "Format: {{\"state\": {{\"_entity_\":\"_value_\"}}, \"confidence\": \"X\"}}\n",
    "Fill the actual entity value into the placeholder encapsulated with underscores.\n",
    "Put \"```\" as EOS token at the end of response.\n",
    "{}\n",
    "Do not capture any other values!\n",
    "If not specified, do not respond to that slot-value.\n",
    "\n",
    "Provide possible entity value based on the last utterance, along with their confidence (0-1).\n",
    "MAKE SURE TO SEPARATE EACH SLOT-VALUE PAIR WITH ITS CONFIDENCE PAIR.\n",
    "Format the output as:\n",
    "```json\n",
    "[\n",
    "  {{\"state\": {{\"_entity1_\":\"_value1_\"}}, \"confidence\": \"X\"}},\n",
    "  {{\"state\": {{\"_entity2_\":\"_value2_\"}}, \"confidence\": \"Y\"}}\n",
    "]``` \n",
    "\n",
    "Now complete the following example, AND PROVIDE CONFIDENCE THAT IT'S CORRECT:\n",
    "input: <|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "{}\n",
    "<|eot_id|>\n",
    "\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "***Output JSON format***\n",
    "Output: ```json\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confidence Score Generation\n",
    "To generate confidence scores during fine-tuning, we assess the difficulty of predicting the dialogue state based on the user utterance and dialogue history. The prompt used for this assessment is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "confidence_prompt = \"\"\"\n",
    "<|begin_of_text|>\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "You are a helpful AI assistant for evaluating the hardness of dialogue state tracking from last user utterance given dialogue history <|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "How difficult would it be for a Language Model to predict the dialogue state from:\n",
    "utterance: Customer: {}\n",
    "given dialogue history\n",
    "history:\n",
    "{}\n",
    "\n",
    "Choose the level of hardness from (Easy/Medium/Hard).\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use four difficulty levels—Easy, Medium, Hard, and Other—and map them to confidence scores with appropriate randomness:\n",
    "\n",
    "* Easy: 0.9 to 1.0\n",
    "* Medium: 0.8 to 0.9\n",
    "* Hard: 0.7 to 0.8\n",
    "* Other: Default score of 0.5\n",
    "\n",
    "This mapping adds variability to the confidence scores, reflecting real-world uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gt_confidence(model, tokenizer, streamer, utterance: str, context: str) -> int:\n",
    "    filled_confidence_prompt = confidence_prompt.format(utterance, context)\n",
    "    # print(filled_confidence_prompt)\n",
    "    confidence_input = tokenizer(filled_confidence_prompt, return_tensors=\"pt\").input_ids.cuda()\n",
    "    outputs = response(model, \"meta-llama/Meta-Llama-3-8B-Instruct\", streamer, confidence_input, temperature=1)\n",
    "    generated_text = tokenizer.decode(outputs['sequences'][0], skip_special_tokens=True)\n",
    "    # print(generated_text)\n",
    "    confidence = generated_text.split(\"Answer:\")[1].lower()\n",
    "    # print(confidence)\n",
    "\n",
    "    if \"easy\" in confidence:\n",
    "        return random.uniform(0.9, 1)\n",
    "    elif \"medium\" in confidence:\n",
    "        return random.uniform(0.8, 0.9)\n",
    "    elif \"hard\" in confidence:\n",
    "        return random.uniform(0.7, 0.8)\n",
    "    else:\n",
    "        return 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing MultiWOZ Turn Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def get_turn_info(dataset):\n",
    "    all_turns = []\n",
    "    for dialog in dataset:\n",
    "        dialog_id = dialog[\"dialogue_id\"].split('.')[0].lower()\n",
    "        \n",
    "        last_state = {}\n",
    "        for tn in range(0, len(dialog[\"turns\"][\"utterance\"]), 2):\n",
    "            context = [f\"Customer: {t}\" if n % 2 == 0 else f\"Assistant: {t}\" for n, t in enumerate(dialog[\"turns\"][\"utterance\"][:tn+1])]\n",
    "            state = dialog[\"turns\"][\"frames\"][tn][\"state\"]\n",
    "            \n",
    "            gt_domain = []\n",
    "            if len(state) == 0:\n",
    "                state = {}\n",
    "            else:\n",
    "                state = [state[i][\"slots_values\"] for i in range(len(state))]\n",
    "                state = [{k: v[0] for k, v in zip(state[i][\"slots_values_name\"], state[i][\"slots_values_list\"])} for i in range(len(state)) if len(state[i][\"slots_values_name\"]) > 0]\n",
    "\n",
    "            new_state = copy.deepcopy(last_state)\n",
    "            for i in range(len(state)):\n",
    "                for sl, val in state[i].items():\n",
    "                    domain, name = sl.split(\"-\")\n",
    "                    if domain not in new_state:\n",
    "                        new_state[domain] = {name: val}\n",
    "                    else:\n",
    "                        new_state[domain][name] = val\n",
    "            state_update = {}\n",
    "            for domain, domain_state in new_state.items():\n",
    "                for slot, value in domain_state.items():\n",
    "                    if slot not in last_state.get(domain, {}) or last_state[domain][slot] != value:\n",
    "                        if domain not in state_update:\n",
    "                            state_update[domain] = {}\n",
    "                        state_update[domain][slot] = value\n",
    "                        \n",
    "            for domain, domain_state in state_update.items():\n",
    "                gt_domain.append(domain)\n",
    "                \n",
    "            last_state = new_state\n",
    "            \n",
    "            confidence_score = gt_confidence(model, tokenizer, streamer, dialog['turns']['utterance'][tn], context)\n",
    "            \n",
    "            turn = {\n",
    "                \"question\": dialog[\"turns\"][\"utterance\"][tn],\n",
    "                \"gt_state\": copy.deepcopy(last_state), # total state\n",
    "                \"dialog_id\": copy.deepcopy(dialog_id),\n",
    "                \"metadata\": {\n",
    "                    \"domain\": copy.deepcopy(gt_domain),\n",
    "                    \"turn_state\": copy.deepcopy(state_update),\n",
    "                    \"total_state\": copy.deepcopy(last_state),\n",
    "                    \"context\": \"\\n\".join(context[-6:])\n",
    "                }\n",
    "            }\n",
    "            all_turns.append(turn)\n",
    "    \n",
    "    return all_turns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Instruction Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from slot_description import DOMAIN_SLOT_DESCRIPTION\n",
    "\n",
    "fine_tuned_confidence = True\n",
    "\n",
    "def generate_instruction_dataset(data_point):\n",
    "    gt_domain = data_point[\"metadata\"][\"domain\"]\n",
    "    context = data_point[\"metadata\"][\"context\"]\n",
    "    utterance = data_point[\"question\"]\n",
    "    turn_state = data_point[\"metadata\"][\"turn_state\"]\n",
    "    \n",
    "    domain_description = \"\"\n",
    "    if gt_domain:\n",
    "        for domain in gt_domain:\n",
    "            domain_description += DOMAIN_SLOT_DESCRIPTION[domain]\n",
    "    \n",
    "    target_str = \"\"\n",
    "    if fine_tuned_confidence:\n",
    "        for domain in turn_state.keys():\n",
    "            for slot, value in turn_state[domain].items():\n",
    "                buf = f\"{{\\\"state\\\": {{\\\"{str(slot)}\\\": \\\"{str(value)}\\\"}}, \\\"confidence\\\": \\\"{str(confidence)}\\\"}},\"\n",
    "                # print(buf)\n",
    "                target_str += buf\n",
    "    else:\n",
    "        for domain in turn_state.keys():\n",
    "            for slot, value in turn_state[domain].items():\n",
    "                buf = \"{\" + \"\\\"\" + \"state\\\": \" + \"{\\\"\" + str(slot) + \"\\\": \\\"\" + str(value) + \"\\\"}}, \"\n",
    "                target_str += buf\n",
    "\n",
    "    if target_str.endswith(\", \"):\n",
    "        target_str = target_str[:-2]\n",
    "\n",
    "    target_str = \"[\" + target_str + \"]\" + \"```\"\n",
    "    if fine_tuned_confidence:\n",
    "        text = \"###Prompt###\" + prompt_confidence.format(domain_description, context) + \"###Completion###\\n\" + target_str + tokenizer.eos_token\n",
    "    else:\n",
    "        text = \"###Prompt###\" + prompt.format(domain_description, context) + \"###Completion###\\n\" + target_str + tokenizer.eos_token\n",
    "    return {\"text\": text, \"labels\": target_str}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_dataset(data):\n",
    "    dataset = []\n",
    "    for i in range(len(data)):\n",
    "        dialog = data[i]\n",
    "        dataset.append(generate_instruction_dataset(dialog))\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading and Preparing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def get_train_valid_data(sample):\n",
    "    dataset = load_dataset(\"multi_woz_v22\")\n",
    "\n",
    "    train_data = dataset[\"train\"]\n",
    "    valid_data = dataset[\"validation\"]\n",
    "\n",
    "    if sample:\n",
    "        train_data = train_data.select([i for i in range(sample['train_size'])])\n",
    "        valid_data = valid_data.select([i for i in range(sample['valid_size'])])\n",
    "\n",
    "    train_turn_data = get_turn_info(train_data)\n",
    "    valid_turn_data = get_turn_info(valid_data)\n",
    "    \n",
    "    train_turn_data = process_dataset(train_turn_data)\n",
    "    valid_turn_data = process_dataset(valid_turn_data)\n",
    "    \n",
    "    return train_turn_data, valid_turn_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample = None\n",
    "# sample = {\"train_size\" : 2000, \"valid_size\":400} \n",
    "train_data, validation_data = get_train_valid_data(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"train_data.json\", \"w\") as file:\n",
    "    json.dump(train_data, file)\n",
    "with open(\"valid_data.json\", \"w\") as file:\n",
    "    json.dump(validation_data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"train_data.json\", \"r\") as file:\n",
    "    train_data = json.load(file)\n",
    "with open(\"valid_data.json\", \"r\") as file:\n",
    "    validation_data = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-Tuning Process\n",
    "During fine-tuning, the model is provided with both the ground truth state and the corresponding confidence score for each slot-value pair. This dual-training approach improves the model's ability to predict dialogue states and verbalize its confidence.\n",
    "\n",
    "By integrating confidence scores, the model becomes better at predicting dialogue states with an expressed level of confidence, enhancing the overall performance and reliability of the dialogue state tracking system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from datasets import Dataset, load_dataset\n",
    "import os\n",
    "import wandb\n",
    "from transformers import TrainerCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomWandbLoggingCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, **kwargs):\n",
    "        if state.is_world_process_zero:\n",
    "            logs = {k: v for k, v in state.log_history[-1].items() if isinstance(v, (int, float))}\n",
    "            wandb.log(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fine_tune_model(model, tokenizer, train_subset, valid_subset, saved_model):\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_subset,\n",
    "        eval_dataset=valid_subset,\n",
    "        peft_config=lora_config,\n",
    "        dataset_text_field='text',\n",
    "        max_seq_length=1500,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_arguments,\n",
    "        callbacks=[CustomWandbLoggingCallback()],\n",
    "    )\n",
    "    trainer.train()\n",
    "    \n",
    "    trainer.model.save_pretrained(saved_model)\n",
    "    tokenizer.save_pretrained(saved_model)\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize Weights & Biases project\n",
    "wandb.init(project=\"llama3-finetuning-DST\", entity=\"jennysun-cs09\", name=\"fullset_epoch1_gtconf\")\n",
    "wandb.config.update(training_arguments)\n",
    "\n",
    "# Convert datasets to Hugging Face Dataset format\n",
    "train_subset = Dataset.from_dict({key: [dic[key] for dic in train_data] for key in train_data[0]})\n",
    "valid_subset = Dataset.from_dict({key: [dic[key] for dic in validation_data] for key in validation_data[0]})\n",
    "\n",
    "# Define the output directory\n",
    "saved_model = os.path.join(\"saved_model_5\", f\"fullset_epoch1_gtconf\")\n",
    "\n",
    "# Fine-tune the model\n",
    "model = fine_tune_model(model, tokenizer, train_subset, valid_subset, saved_model)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:llama-new]",
   "language": "python",
   "name": "conda-env-llama-new-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
